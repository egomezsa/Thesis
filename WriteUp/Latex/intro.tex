\renewcommand{\chaptername}{}

\chapter{INTRODUCTION}

In recent years, music based services have been trying to make their 
applications more user-centered. One way to make sure that the 
user experience is foremost, is to provide services that match the user's 
current emotional state. This can be implemented by understanding 
the emotional content of specific songs and playlists. 

Sentiment analysis is the portion of Music Informational Retrieval (MIR) 
where an algorithm recognizes the main emotions that that a song evokes. 
Emotions are subjective, so classifying media into distinct groups 
is a challenging problem. Most human subjects agree in broad 
strokes on emotional classifications. However it is not uncommon
 to find media inputs where there is no consensus, leading to 
 inconsistencies in class groupings. As a result, sentiment analysis 
 is challenging to incorporate into real-world applications 
 because an algorithm's recognition accuracy might differ between classes. 
 
  The motivation behind this research is to find a method that accounts for 
  these inter-class inconsistencies across a large dataset.  During initial 
  testing it became apparent that certain classifiers perform well on specific 
  sentiments and fail to learn features that represent others. This 
  research seeks to account for this difference by combining the classifiers 
  to output classifications in a  more consistent manner, thus improving the 
  reliability of the overall system. 
  
  The dataset used for this paper is called the Million Song Dataset, and 
  was compiled by Labrosa [I.E]. This dataset contains a million different songs
  represented by their pitch, loudness and timbre. The songs are also accompanied 
  by plenty of metadata such as tags. Sentiment classification was obtained from
  these tags. If a sentiment was used to described that song, it is assumed that 
  the song conveyed that sentiment. The lyric information was obtained from the
  musicXmatch dataset  [II.F], which provides lyric information out of order in a 
  bag-of-words format. Since there was no way to obtain semantic information 
  from an unordered bag-of-words representation, this research did not focus 
  on the impact of semantics on sentiment classification. 
      
 This document starts with a brief overview of previous work performed on this 
 topic, followed by a description of the algorithms developed for the experiments, 
 then a section containing results and corresponding analysis and closes 
 with conclusions and suggestions for future work.
 
 \chapter{PREVIOUS WORK}

There has been considerable amount of work done in the field of multimodal 
sentiment analysis. This section will briefly cover a portion of the relevant 
research that was considered during the development of the presented 
methodology. The relevant topics that were research for this paper were: Audio Sentiment 
Analysis, Text Sentiment Analysis and Multimodal Classification.


\section{Audio Sentiment Analysis}

The study of the relationship between emotional content and audio signals is a 
very mature field. Researchers have expanded on the success found in the 
speech recognition community while using Mel-Frequency Cepstral Components 
(MFCC) to explore their uses in music modeling [I.A]. MFCCs are currently a 
staple in audio processing and are commonly used in MIR applications such 
as genre classification [I.B], since it is a quantifiable method for comparing the 
timbral texture of songs. Timbre has been successfully used to classify the 
emotional content of songs [I.C]. It has also been used to generate 
songs that evoke particular emotions [I.D].  These vectors have been commonly 
classified using Support Vector Machines (SVM) and Naive Bayes classifiers. The 
dataset used for these experiments is the Million Song Dataset (MSDS) [I.E] 
since metadata provided is intended for MIR research. The audio features 
used were the MFCC-like timbre vectors provided by EchoNest in the MSDS. 

\section{Text Sentiment Analysis}

Similarly the study of the relationship between text and emotional content is quite 
developed. From predicting Yelp ratings based on the sentiment expressed on the 
review [II.A]  to extracting the emotional progression of major literary pieces [I.D].  
There are many methods to represent and extract emotional information from texts. 
The Yelp experiment uses a statistical word vectors to capture word semantics and 
emotions as a probability.  Other researchers have represented textual information in 
a bag-of-features framework and classified them using Naive Bayes, SVMs and 
Maximum Entropy classifiers to recognize positive or negative valances (II.B).  

Capturing the semantic nuisances has also been an area of great interest, which is 
the study of how a given word might have different emotional value depending on its 
context. This level of analysis requires the creation of complex sentiment vectors 
that encode how meanings change based on semantics [II.C].  Similarly researchers 
have improved classification accuracy by preprocessing text [II.D], and use the 
cleaned data to capture emotional subtleties like the use of negation and modifiers 
to emotional words [II.E]. 

Although there is a great body of research on how to obtain rich sentiment vectors 
from the text, the goal of this paper is demonstrate the added advantage of a 
multimodal approach. As a result, the features used will be the bag-of-words vector 
provided by the musicXatch dataset [II.F].

\section{Multimodal Classification}

Multimodal classification is the task of using feature vectors from different spaces, 
for example text and audio, to reach a single classification. There are two main 
methods of combining the information from both vector spaces: feature fusion and 
classifier fusion [III.A]. 

Feature Fusion is the technique that takes signals from different 
feature spaces and joins them to train a single multimodal classifier. The standard 
fusion method is called series fusion, which consists on concatenating the vectors 
together and training the classifier on the union of both spaces. Several alternatives 
have been suggested to maintain the same amount of expressibility in the vector while 
keeping the vector space as small as possible. Instead of concatenating the vectors 
together, it is possible to join vectors in parallel [III.B] by making vectors from the linear
combinations of a real-valued feature with another complex-valued feature. The benefit 
of the series fusion over the parallel method is that many diverse features can be fused
together to obtain more robust data. As seen in the research by Liang et al., 
genre classification was improved by joining five different vectors all resulting from 
different preprocessing methods for text and audio vectors [III.C]. 

Classifier fusions consists on training an array of unimodal classifiers and using some 
function to consolidate the predictions [III.D]. This method seamlessly fuses features 
from very different spaces. Caridakis et al. combined facial expressions, body gestures 
and speech by having a classifier voting system where the class with most votes and 
higher probability was chosen amongst all the decisions [III.E]. The final decision-making 
process can be taken a step further by adding an additional classifier that learns from 
the decisions provided from classifier array [III.F].  The algorithms presented on this 
paper were largely based on this last approach. 



\chapter{Motivation}

Multimodal classification has been successful in improving the accuracy of classification 
[III.G, III.H]. Some of the previous work either ran the experiments on highly homogenous 
datasets where all the music was in the same language, belonged to the same genre or 
were carefully classified by a single subject thus eliminating class inconsistencies.  


