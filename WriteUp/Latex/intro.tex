\renewcommand{\chaptername}{}

\chapter{INTRODUCTION}

In recent years, music-based services have been trying to make their 
applications more user-centered. One way to make sure that the 
user experience is foremost, is to provide services that match the user's 
current emotional state. This can be implemented by understanding 
the emotional content of the songs and playlists that are being played. 

Sentiment analysis is the portion of music informational retrieval (MIR) 
where an algorithm recognizes the main emotions that a song evokes. 
Emotions are subjective, so classifying individual songs into distinct groups 
is a challenging problem. Most human subjects agree in broad 
strokes on emotional classifications. However it is not uncommon
 to find songs where there is no consensus, leading to 
 inconsistencies in class groupings. As a result, sentiment analysis 
 is still an open problem that requires the investigation of alternative methods that 
 employ different modalities to counteract class inconsistencies.  Success in 
 this approach would allow a more robust classification algorithm 
 that is better suited for real-world applications.
  
  The motivation behind this research is to find a method that accounts for 
  these inter-class inconsistencies across a large dataset.  During initial 
  testing it became apparent that certain classifiers perform well on specific 
  sentiments and fail to learn features that represent others. This 
  research seeks to account for this difference by combining the classifiers 
  to output classifications in a  more consistent manner, thus improving the 
  reliability of the overall system.
  
  This thesis will be focusing on the employment of support vector machines (SVM), 
  Gaussian naive bayes and multinomial naive bayes classifiers to recognize
  emotional information.  The audio features that will be employed are based on mel-frequency cepstral coefficients (MFCC),
  which are obtained from the response of the song's windowed spectrogram 
  to a set of basis functions.  A bag-of-words vector is used to represent the lyric portion of the song.  The modal 
  fusion will be tested through both feature fusion and classifier fusion.
  
  The dataset used for this thesis is called the Million Song Dataset, and 
  was compiled by Labrosa \cite{Bertin-Mahieux2011}. This dataset contains a million different songs
  represented by their pitch, loudness, and timbre. The songs are also accompanied 
  by much metadata such as artist, release date, and  tags. Sentiment classification
   was obtained from these tags. If a sentiment was used to describe a song, it is assumed that 
  the song conveyed that sentiment. The lyric information was obtained from the
  MusicXMatch dataset  \cite{musicXmatchDataset}, which provides lyric information out of order in a 
  bag-of-words format. Since there was no way to obtain semantic information 
  from an unordered bag-of-words representation, this research did not focus 
  on the impact of semantics on sentiment classification. 
      
 This report starts with a brief overview of previous work performed on this 
 topic, followed by a description of the methods and the algorithms developed,
 then a section detailing the actual experiments,  a section on results and  analysis.
 It closes with a section on conclusions and suggestions for future work.
 
 \chapter{PREVIOUS WORK}

There has been considerable amount of work done in the field of multimodal 
sentiment analysis. This Chapter will briefly cover a portion of the relevant 
research that was considered during the development of the presented 
methodologies. The relevant topics that were researched for this thesis were: audio sentiment 
analysis, text sentiment analysis, and multimodal classification.


\section*{Audio Sentiment Analysis}

The study of the relationship between emotional content and audio signals is a 
very mature field. Researchers have expanded on the success found in the 
speech recognition community while using mel-frequency cepstral coefficients 
(MFCC) to explore their uses in music modeling \cite{Logan00melfrequency}. MFCCs are currently a 
staple in audio processing and are commonly used in MIR applications such 
as genre classification \cite{Tzanetakis01automaticmusical}, since they are a quantifiable method for comparing the 
timbral texture of songs. Timbre has been used with some success to classify the 
emotional content of songs \cite{University03detectingemotion}; however, class inconsistencies proved to be 
a difficult challenge that creates substantial misclassification between edge cases. 
Timbre has also been used to generate songs that evoke particular emotions \cite{transprose}.  
These vectors have been commonly classified using 
support vector machines (SVM) and naive Bayes classifiers. 

\section*{Text Sentiment Analysis}

Similarly, the study of the relationship between text and emotional content is quite 
developed, applications range from predicting Yelp ratings based on the sentiment expressed on the 
review \cite{YelpReview}  to extracting the emotional progression of major literary pieces \cite{transprose}.  
There are many methods to represent and extract emotional information from texts. 
The Yelp experiment uses statistical word vectors to capture word semantics and 
emotions as a probability.  Other researchers have represented textual information in 
a bag-of-features framework and classified them using naive Bayes, SVMs and 
maximum entropy classifiers to recognize positive or negative valances \cite{Pang:2002:TUS:1118693.1118704}.  

Researchers have extended text processing methods to better capture emotional subtleties. For example, a word can have 
different emotional values depending on its context.  Analyzing this information requires the creation of complex sentiment vectors 
that encode how meanings change based on semantics \cite{Maas:2011:LWV:2002472.2002491}.  Similarly researchers 
have improved classification accuracy by preprocessing text \cite{Haddi201326}, and using the 
cleaned data to capture emotional subtleties like the use of negation and modifiers 
to emotional words \cite{Xia_sentimentvector}. 

Although there is a great body of research on how to obtain rich sentiment vectors 
from text, the goal of this thesis is demonstrate the added advantage of a 
multimodal approach.  For that end, the lyric vectors were kept simple to clearly
underline the benefit of combining them with audio information.  In addition, it is necessary 
to point out that obtaining a large enough dataset of lyrics is difficult due to legal restrictions.   
As a result, the features used will be the unordered representation of the lyrics in 
a bag-of-words vector provided by the MusicXMatch dataset \cite{musicXmatchDataset}.

\section*{Multimodal Classification}

Multimodal classification is the task of using feature vectors from different spaces, 
for example text and audio, to reach a single classification. There are two main 
methods of combining the information from both vector spaces: feature fusion and 
classifier fusion \cite{zhonga2012music}. 

Feature fusion is the technique that takes signals from different 
feature spaces and joins them to train a single multimodal classifier. The standard 
fusion method is called "series fusion", which consists of concatenating the vectors 
together and training the classifier on the union of both spaces. Several alternatives 
have been suggested to maintain the same amount of expressibility in the vector while 
keeping the vector space as small as possible. Instead of concatenating the vectors 
together, it is possible to join vectors in parallel \cite{yang2003feature} by making vectors from the linear
combinations of a real-valued feature with another complex-valued feature. The benefit 
of the series fusion over the parallel method is that many diverse features can be fused
together to obtain more robust data. As seen in the research by Liang et al., 
genre classification was improved by joining five different vectors all resulting from 
different preprocessing methods for text and audio vectors \cite{liang2011music}. 

Classifier fusions train an array of unimodal classifiers and using some 
function to consolidate the predictions \cite{ahmadian2013multi}. This method seamlessly fuses features 
from very different spaces. Caridakis et al. combined facial expressions, body gestures, 
and speech by having a classifier voting system where the class with most votes and 
higher probability was chosen amongst all the decisions \cite{caridakis2007multimodal}. The final decision-making 
process can be taken a step further by adding an additional classifier that learns from 
the decisions provided from classifier array \cite{li2008multi}.  The algorithms presented in this 
research were largely based on this last approach. 


Multimodal classification has been successful in improving the accuracy of classification 
 \cite{zhonga2012music} \cite{hu2010improving}.  However, some of the previous work either ran the experiments on highly homogenous 
datasets where all the music was in the same language, belonged to the same genre, or 
were carefully classified by a single subject thus eliminating class inconsistencies.  The goal of 
this research is to obtain improved classification and reliability from a varied dataset 
through ensemble classifiers. 


